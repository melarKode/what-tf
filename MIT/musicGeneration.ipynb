{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f76182b4c67eda9e9be9e2b182aeb8560210f37ed8d48fe08cf5af4d01ee2941"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: mitdeeplearning in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.1.2)\nRequirement already satisfied: tqdm in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from mitdeeplearning) (4.47.0)\nRequirement already satisfied: regex in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from mitdeeplearning) (2020.6.8)\nRequirement already satisfied: numpy in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from mitdeeplearning) (1.18.5)\nRequirement already satisfied: gym in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from mitdeeplearning) (0.17.3)\nRequirement already satisfied: scipy in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym-&gt;mitdeeplearning) (1.4.1)\nRequirement already satisfied: pyglet&lt;=1.5.0,&gt;=1.4.0 in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym-&gt;mitdeeplearning) (1.5.0)\nRequirement already satisfied: cloudpickle&lt;1.7.0,&gt;=1.2.0 in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gym-&gt;mitdeeplearning) (1.6.0)\nRequirement already satisfied: future in c:\\users\\navne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym-&gt;mitdeeplearning) (0.18.2)\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "!pip install mitdeeplearning\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 816 songs in text\n\nExample song:\nX:2\nT:An Buachaill Dreoite\nZ: id:dc-hornpipe-2\nM:C|\nL:1/8\nK:G Major\nGF|DGGB d2GB|d2GF Gc (3AGF|DGGB d2GB|dBcA F2GF|!\nDGGB d2GF|DGGF G2Ge|fgaf gbag|fdcA G2:|!\nGA|B2BG c2cA|d2GF G2GA|B2BG c2cA|d2DE F2GA|!\nB2BG c2cA|d^cde f2 (3def|g2gf gbag|fdcA G2:|!\n"
    }
   ],
   "source": [
    "songs = mdl.lab1.load_training_data()\n",
    "\n",
    "example_songs = songs[0]\n",
    "print(\"\\nExample song:\")\n",
    "print(example_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.lab1.play_song(example_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_joined = \"\\n\\n\".join(songs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 83 unique characters in the dataset\n"
    }
   ],
   "source": [
    "vocab = sorted(set(songs_joined))\n",
    "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{\n  &#39;\\n&#39;:   0,\n  &#39; &#39; :   1,\n  &#39;!&#39; :   2,\n  &#39;&quot;&#39; :   3,\n  &#39;#&#39; :   4,\n  &quot;&#39;&quot; :   5,\n  &#39;(&#39; :   6,\n  &#39;)&#39; :   7,\n  &#39;,&#39; :   8,\n  &#39;-&#39; :   9,\n  &#39;.&#39; :  10,\n  &#39;/&#39; :  11,\n  &#39;0&#39; :  12,\n  &#39;1&#39; :  13,\n  &#39;2&#39; :  14,\n  &#39;3&#39; :  15,\n  &#39;4&#39; :  16,\n  &#39;5&#39; :  17,\n  &#39;6&#39; :  18,\n  &#39;7&#39; :  19,\n   ...\n\n"
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char),char2idx[char]))\n",
    "print('   ...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_string(string):\n",
    "    vectorized_output = np.array([char2idx[char] for char in string])\n",
    "    return vectorized_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_songs = vectorize_string(songs_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "&#39;X:2\\nT:An B&#39; ---- characters mapped to int ----&gt; [49 22 14  0 45 22 26 69  1 27]\n"
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
    "# check that vectorized_songs is a numpy array\n",
    "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[PASS] test_batch_func_types\n[PASS] test_batch_func_shapes\n[PASS] test_batch_func_next_step\n======\n[PASS] passed all tests!\n"
    }
   ],
   "source": [
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "    n = vectorized_songs.shape[0]-1\n",
    "    idx = np.random.choice(n-seq_length, batch_size)\n",
    "\n",
    "    input_batch = [vectorized_songs[i:i+seq_length] for i in idx]\n",
    "    output_batch = [vectorized_songs[i+1: i+seq_length+1] for i in idx]\n",
    "\n",
    "    x_batch = np.reshape(input_batch,[batch_size, seq_length])\n",
    "    y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
    "    return x_batch, y_batch\n",
    "\n",
    "test_args = (vectorized_songs, 10, 2)\n",
    "if not mdl.lab1.test_batch_func_types(get_batch, test_args) or \\\n",
    "   not mdl.lab1.test_batch_func_shapes(get_batch, test_args) or \\\n",
    "   not mdl.lab1.test_batch_func_next_step(get_batch, test_args): \n",
    "   print(\"======\\n[FAIL] could not pass tests\")\n",
    "else: \n",
    "   print(\"======\\n[PASS] passed all tests!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Step   0\n  input: 56 (&#39;a&#39;)\n  expected output: 62 (&#39;g&#39;)\nStep   1\n  input: 62 (&#39;g&#39;)\n  expected output: 57 (&#39;b&#39;)\nStep   2\n  input: 57 (&#39;b&#39;)\n  expected output: 1 (&#39; &#39;)\nStep   3\n  input: 1 (&#39; &#39;)\n  expected output: 56 (&#39;a&#39;)\nStep   4\n  input: 56 (&#39;a&#39;)\n  expected output: 14 (&#39;2&#39;)\n"
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(rnn_units):\n",
    "    return tf.keras.layers.LSTM(\n",
    "        rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        recurrent_activation='sigmoid',\n",
    "        stateful=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size,None]),\n",
    "        LSTM(rnn_units),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(len(vocab), embedding_dim=256,rnn_units=1024, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (32, None, 256)           21248     \n_________________________________________________________________\nlstm (LSTM)                  (32, None, 1024)          5246976   \n_________________________________________________________________\ndense (Dense)                (32, None, 83)            85075     \n=================================================================\nTotal params: 5,353,299\nTrainable params: 5,353,299\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input shape:       (32, 100)  # (batch_size, sequence_length)\nPrediction shape:  (32, 100, 83) # (batch_size, sequence_length, vocab_size)\n"
    }
   ],
   "source": [
    "x,y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
    "pred = model(x)\n",
    "print(\"Input shape:      \",x.shape,\" # (batch_size, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([49, 55,  0, 63, 54, 81, 12, 76, 80, 29, 51, 64,  1, 63, 77, 62, 66,\n       72,  4, 13, 16, 49, 24, 37, 58, 40, 60, 47, 24, 60, 57,  1, 24,  6,\n       31, 35, 52, 51, 13, 19, 30, 13, 22, 29, 51, 52, 62,  4, 37,  8, 78,\n        4, 33, 70,  6, 15, 32,  9, 72, 25, 79,  0,  5, 21, 79, 27, 28, 81,\n       21, 60, 59, 49, 27, 46, 13,  0, 57, 17, 41, 63, 60, 70, 48,  8, 26,\n       50,  2, 24, 79, 30, 62, 43, 34, 32, 39, 39, 60, 35, 79,  5],\n      dtype=int64)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input: \n &#39;|BA Bc|ed d:|!\\ne|f2 e2|d2-d&gt;c|BA Bc|BA FA|!\\nf2 e2|d2-d&gt;c|BA Bc|ed d:|!\\n\\nX:48\\nT:Where Lilies Bloom\\nZ:&#39;\n\nNext Char Predictions: \n &quot;X_\\nh^z0uyDZi hvgkq#14X=LcOeV=eb =(FJ[Z17E1:DZ[g#L,w#Ho(3G-q&gt;x\\n&#39;9xBCz9edXBU1\\nb5PheoW,AY!=xEgRIGNNeJx&#39;&quot;\n"
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction shape:  (32, 100, 83)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.419907\n"
    }
   ],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    return loss\n",
    "example_batch_loss = compute_loss(y,pred)\n",
    "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 2000\n",
    "batch_size = 4\n",
    "seq_length = 100\n",
    "learning_rate = 5e-3\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = compute_loss(y,y_hat)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name &#39;num_training_iterations&#39; is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m&lt;ipython-input-23-21f6a78c668a&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m&#39;_instances&#39;\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_instances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clear if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----&gt; 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_training_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m   \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorized_songs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name &#39;num_training_iterations&#39; is not defined"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "for iter in tqdm(range(num_iterations)):\n",
    "  x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
    "  loss = train_step(x_batch, y_batch)\n",
    "  history.append(loss.numpy().mean())\n",
    "  plotter.plot(history)\n",
    "  if iter % 100 == 0:     \n",
    "    model.save_weights(checkpoint_prefix)\n",
    "model.save_weights(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}